{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1cc64f2",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe644e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import os\n",
    "from numpy import sort\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from collections import Counter\n",
    "import math \n",
    "from gensim.models import Word2Vec\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b7bfb6",
   "metadata": {},
   "source": [
    "# EDA and Data Pre-processing\n",
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1081bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "trl = pd.read_csv('train_label.csv')\n",
    "trb = pd.read_csv('train_base.csv')\n",
    "tro = pd.read_csv('train_op.csv')\n",
    "trt = pd.read_csv('train_trans.csv')\n",
    "\n",
    "teb = pd.read_csv('testb_base.csv')\n",
    "teo = pd.read_csv('testb_op.csv')\n",
    "tet = pd.read_csv('testb_trans.csv')\n",
    "\n",
    "trbl = pd.merge(trb, trl, on = 'user', how = 'left')\n",
    "data = pd.concat([trbl, teb], axis=0, ignore_index=True)\n",
    "\n",
    "op = pd.concat([tro, teo], axis=0, ignore_index=True)\n",
    "trans = pd.concat([trt, tet], axis=0, ignore_index=True)\n",
    "\n",
    "del trl, trb, tro, trt, teb, teo, tet, trbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa3a8b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>op_type</th>\n",
       "      <th>op_mode</th>\n",
       "      <th>op_device</th>\n",
       "      <th>ip</th>\n",
       "      <th>net_type</th>\n",
       "      <th>channel</th>\n",
       "      <th>ip_3</th>\n",
       "      <th>tm_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train_36517</td>\n",
       "      <td>b26bc49195bd79cf</td>\n",
       "      <td>87ee0bdf333a54da</td>\n",
       "      <td>92dc8b3f9a9ea13b</td>\n",
       "      <td>bbb0af60b941370b</td>\n",
       "      <td>116a2503b987ea81</td>\n",
       "      <td>4e1ff124e1e6adc8</td>\n",
       "      <td>1e46c177cd9d539a</td>\n",
       "      <td>11 days 09:38:22.000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Train_36517</td>\n",
       "      <td>b26bc49195bd79cf</td>\n",
       "      <td>87ee0bdf333a54da</td>\n",
       "      <td>92dc8b3f9a9ea13b</td>\n",
       "      <td>bbb0af60b941370b</td>\n",
       "      <td>116a2503b987ea81</td>\n",
       "      <td>4e1ff124e1e6adc8</td>\n",
       "      <td>1e46c177cd9d539a</td>\n",
       "      <td>11 days 09:38:21.000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Train_36517</td>\n",
       "      <td>b26bc49195bd79cf</td>\n",
       "      <td>87ee0bdf333a54da</td>\n",
       "      <td>92dc8b3f9a9ea13b</td>\n",
       "      <td>bbb0af60b941370b</td>\n",
       "      <td>116a2503b987ea81</td>\n",
       "      <td>4e1ff124e1e6adc8</td>\n",
       "      <td>1e46c177cd9d539a</td>\n",
       "      <td>11 days 09:38:23.000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Train_36517</td>\n",
       "      <td>b26bc49195bd79cf</td>\n",
       "      <td>87ee0bdf333a54da</td>\n",
       "      <td>92dc8b3f9a9ea13b</td>\n",
       "      <td>bbb0af60b941370b</td>\n",
       "      <td>116a2503b987ea81</td>\n",
       "      <td>4e1ff124e1e6adc8</td>\n",
       "      <td>1e46c177cd9d539a</td>\n",
       "      <td>11 days 09:38:26.000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Train_36517</td>\n",
       "      <td>b26bc49195bd79cf</td>\n",
       "      <td>87ee0bdf333a54da</td>\n",
       "      <td>92dc8b3f9a9ea13b</td>\n",
       "      <td>bbb0af60b941370b</td>\n",
       "      <td>116a2503b987ea81</td>\n",
       "      <td>4e1ff124e1e6adc8</td>\n",
       "      <td>1e46c177cd9d539a</td>\n",
       "      <td>11 days 09:38:41.000000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          user           op_type           op_mode         op_device  \\\n",
       "0  Train_36517  b26bc49195bd79cf  87ee0bdf333a54da  92dc8b3f9a9ea13b   \n",
       "1  Train_36517  b26bc49195bd79cf  87ee0bdf333a54da  92dc8b3f9a9ea13b   \n",
       "2  Train_36517  b26bc49195bd79cf  87ee0bdf333a54da  92dc8b3f9a9ea13b   \n",
       "3  Train_36517  b26bc49195bd79cf  87ee0bdf333a54da  92dc8b3f9a9ea13b   \n",
       "4  Train_36517  b26bc49195bd79cf  87ee0bdf333a54da  92dc8b3f9a9ea13b   \n",
       "\n",
       "                 ip          net_type           channel              ip_3  \\\n",
       "0  bbb0af60b941370b  116a2503b987ea81  4e1ff124e1e6adc8  1e46c177cd9d539a   \n",
       "1  bbb0af60b941370b  116a2503b987ea81  4e1ff124e1e6adc8  1e46c177cd9d539a   \n",
       "2  bbb0af60b941370b  116a2503b987ea81  4e1ff124e1e6adc8  1e46c177cd9d539a   \n",
       "3  bbb0af60b941370b  116a2503b987ea81  4e1ff124e1e6adc8  1e46c177cd9d539a   \n",
       "4  bbb0af60b941370b  116a2503b987ea81  4e1ff124e1e6adc8  1e46c177cd9d539a   \n",
       "\n",
       "                      tm_diff  \n",
       "0  11 days 09:38:22.000000000  \n",
       "1  11 days 09:38:21.000000000  \n",
       "2  11 days 09:38:23.000000000  \n",
       "3  11 days 09:38:26.000000000  \n",
       "4  11 days 09:38:41.000000000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>platform</th>\n",
       "      <th>tunnel_in</th>\n",
       "      <th>tunnel_out</th>\n",
       "      <th>amount</th>\n",
       "      <th>type1</th>\n",
       "      <th>ip</th>\n",
       "      <th>type2</th>\n",
       "      <th>ip_3</th>\n",
       "      <th>tm_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train_13770</td>\n",
       "      <td>46c69cbbce5f1568</td>\n",
       "      <td>b2e7fa260df4998d</td>\n",
       "      <td>6ee790756007e69a</td>\n",
       "      <td>185784</td>\n",
       "      <td>45a1168437c708ff</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11a213398ee0c623</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19 days 09:02:45.000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Train_13770</td>\n",
       "      <td>46c69cbbce5f1568</td>\n",
       "      <td>b2e7fa260df4998d</td>\n",
       "      <td>6ee790756007e69a</td>\n",
       "      <td>391769</td>\n",
       "      <td>45a1168437c708ff</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11a213398ee0c623</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19 days 09:03:58.000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Train_08351</td>\n",
       "      <td>46c69cbbce5f1568</td>\n",
       "      <td>b2e7fa260df4998d</td>\n",
       "      <td>6ee790756007e69a</td>\n",
       "      <td>27740</td>\n",
       "      <td>674e8d5860bc033d</td>\n",
       "      <td>f10a09fe9e522a47</td>\n",
       "      <td>11a213398ee0c623</td>\n",
       "      <td>ee386d6f9fe45d0d</td>\n",
       "      <td>18 days 11:06:49.000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Train_08351</td>\n",
       "      <td>42573d7287a8c9c2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6ee790756007e69a</td>\n",
       "      <td>36689</td>\n",
       "      <td>f67d4b5a05a1352a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26 days 09:52:51.000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Train_08351</td>\n",
       "      <td>42573d7287a8c9c2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6ee790756007e69a</td>\n",
       "      <td>30746</td>\n",
       "      <td>f67d4b5a05a1352a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26 days 07:50:05.000000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          user          platform         tunnel_in        tunnel_out  amount  \\\n",
       "0  Train_13770  46c69cbbce5f1568  b2e7fa260df4998d  6ee790756007e69a  185784   \n",
       "1  Train_13770  46c69cbbce5f1568  b2e7fa260df4998d  6ee790756007e69a  391769   \n",
       "2  Train_08351  46c69cbbce5f1568  b2e7fa260df4998d  6ee790756007e69a   27740   \n",
       "3  Train_08351  42573d7287a8c9c2               NaN  6ee790756007e69a   36689   \n",
       "4  Train_08351  42573d7287a8c9c2               NaN  6ee790756007e69a   30746   \n",
       "\n",
       "              type1                ip             type2              ip_3  \\\n",
       "0  45a1168437c708ff               NaN  11a213398ee0c623               NaN   \n",
       "1  45a1168437c708ff               NaN  11a213398ee0c623               NaN   \n",
       "2  674e8d5860bc033d  f10a09fe9e522a47  11a213398ee0c623  ee386d6f9fe45d0d   \n",
       "3  f67d4b5a05a1352a               NaN               NaN               NaN   \n",
       "4  f67d4b5a05a1352a               NaN               NaN               NaN   \n",
       "\n",
       "                      tm_diff  \n",
       "0  19 days 09:02:45.000000000  \n",
       "1  19 days 09:03:58.000000000  \n",
       "2  18 days 11:06:49.000000000  \n",
       "3  26 days 09:52:51.000000000  \n",
       "4  26 days 07:50:05.000000000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>provider</th>\n",
       "      <th>level</th>\n",
       "      <th>verified</th>\n",
       "      <th>using_time</th>\n",
       "      <th>regist_type</th>\n",
       "      <th>card_a_cnt</th>\n",
       "      <th>card_b_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>service3_level</th>\n",
       "      <th>product1_amount</th>\n",
       "      <th>product2_amount</th>\n",
       "      <th>product3_amount</th>\n",
       "      <th>product4_amount</th>\n",
       "      <th>product5_amount</th>\n",
       "      <th>product6_amount</th>\n",
       "      <th>product7_cnt</th>\n",
       "      <th>product7_fail_cnt</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train_06800</td>\n",
       "      <td>category 1</td>\n",
       "      <td>24877</td>\n",
       "      <td>category 0</td>\n",
       "      <td>category 2</td>\n",
       "      <td>category 0</td>\n",
       "      <td>24728</td>\n",
       "      <td>category 1</td>\n",
       "      <td>24712</td>\n",
       "      <td>24712</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>level 1</td>\n",
       "      <td>level 1</td>\n",
       "      <td>level 1</td>\n",
       "      <td>level 0</td>\n",
       "      <td>level 0</td>\n",
       "      <td>level 1</td>\n",
       "      <td>24706</td>\n",
       "      <td>24706</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Train_23487</td>\n",
       "      <td>category 1</td>\n",
       "      <td>24895</td>\n",
       "      <td>category 0</td>\n",
       "      <td>category 2</td>\n",
       "      <td>category 0</td>\n",
       "      <td>24719</td>\n",
       "      <td>category 1</td>\n",
       "      <td>24719</td>\n",
       "      <td>24719</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>level 1</td>\n",
       "      <td>level 1</td>\n",
       "      <td>level 1</td>\n",
       "      <td>level 0</td>\n",
       "      <td>level 0</td>\n",
       "      <td>level 1</td>\n",
       "      <td>24712</td>\n",
       "      <td>24706</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Train_36880</td>\n",
       "      <td>category 0</td>\n",
       "      <td>24853</td>\n",
       "      <td>category 0</td>\n",
       "      <td>category 2</td>\n",
       "      <td>category 0</td>\n",
       "      <td>24723</td>\n",
       "      <td>category 1</td>\n",
       "      <td>24719</td>\n",
       "      <td>24719</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>level 1</td>\n",
       "      <td>level 1</td>\n",
       "      <td>level 1</td>\n",
       "      <td>level 0</td>\n",
       "      <td>level 0</td>\n",
       "      <td>level 1</td>\n",
       "      <td>24712</td>\n",
       "      <td>24706</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Train_35392</td>\n",
       "      <td>category 0</td>\n",
       "      <td>24938</td>\n",
       "      <td>category 1</td>\n",
       "      <td>category 1</td>\n",
       "      <td>category 0</td>\n",
       "      <td>24707</td>\n",
       "      <td>category 3</td>\n",
       "      <td>24712</td>\n",
       "      <td>24712</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>level 1</td>\n",
       "      <td>level 1</td>\n",
       "      <td>level 1</td>\n",
       "      <td>level 0</td>\n",
       "      <td>level 0</td>\n",
       "      <td>level 1</td>\n",
       "      <td>24712</td>\n",
       "      <td>24706</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Train_35057</td>\n",
       "      <td>category 0</td>\n",
       "      <td>24956</td>\n",
       "      <td>category 0</td>\n",
       "      <td>category 2</td>\n",
       "      <td>category 0</td>\n",
       "      <td>24728</td>\n",
       "      <td>category 2</td>\n",
       "      <td>24719</td>\n",
       "      <td>24712</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>level 2</td>\n",
       "      <td>level 8</td>\n",
       "      <td>level 1</td>\n",
       "      <td>level 0</td>\n",
       "      <td>level 0</td>\n",
       "      <td>level 1</td>\n",
       "      <td>24712</td>\n",
       "      <td>24706</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          user         sex    age    provider       level    verified  \\\n",
       "0  Train_06800  category 1  24877  category 0  category 2  category 0   \n",
       "1  Train_23487  category 1  24895  category 0  category 2  category 0   \n",
       "2  Train_36880  category 0  24853  category 0  category 2  category 0   \n",
       "3  Train_35392  category 0  24938  category 1  category 1  category 0   \n",
       "4  Train_35057  category 0  24956  category 0  category 2  category 0   \n",
       "\n",
       "   using_time regist_type  card_a_cnt  card_b_cnt  ...  service3_level  \\\n",
       "0       24728  category 1       24712       24712  ...             NaN   \n",
       "1       24719  category 1       24719       24719  ...             NaN   \n",
       "2       24723  category 1       24719       24719  ...             NaN   \n",
       "3       24707  category 3       24712       24712  ...             NaN   \n",
       "4       24728  category 2       24719       24712  ...             NaN   \n",
       "\n",
       "  product1_amount  product2_amount  product3_amount  product4_amount  \\\n",
       "0         level 1          level 1          level 1          level 0   \n",
       "1         level 1          level 1          level 1          level 0   \n",
       "2         level 1          level 1          level 1          level 0   \n",
       "3         level 1          level 1          level 1          level 0   \n",
       "4         level 2          level 8          level 1          level 0   \n",
       "\n",
       "   product5_amount  product6_amount  product7_cnt  product7_fail_cnt label  \n",
       "0          level 0          level 1         24706              24706   0.0  \n",
       "1          level 0          level 1         24712              24706   1.0  \n",
       "2          level 0          level 1         24712              24706   1.0  \n",
       "3          level 0          level 1         24712              24706   0.0  \n",
       "4          level 0          level 1         24712              24706   1.0  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op.head()\n",
    "trans.head()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b29cda8",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Time + Amount + Recency + Equipment + Geography + Stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "498cc6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na_zero(nadata, col1, col2):\n",
    "    idx = nadata[nadata[col2]==0].index\n",
    "    nadata.loc[idx, col1] = 0\n",
    "    return nadata\n",
    "\n",
    "\n",
    "def fill_discrete_conti_na(nadata):\n",
    "    i = 0\n",
    "    for col in nadata.columns:\n",
    "        i += 1\n",
    "        if i%10 == 0:\n",
    "            #print(i)\n",
    "        if col in ['user', 'label']:\n",
    "            continue\n",
    "        if nadata[col].dtype in ['int64','float64', 'int32', 'float32']:\n",
    "            num = len(set(nadata[col]))\n",
    "            if num < 10: # 离散型\n",
    "                #print(col)\n",
    "                nadata[col] = nadata[col].fillna(nadata[col].mode())\n",
    "                nadata[col] = nadata[col].astype(float)\n",
    "            else:\n",
    "                nadata[col] = nadata[col].fillna(nadata[col].median())\n",
    "                nadata[col] = nadata[col].astype(float)\n",
    "    return nadata  \n",
    "\n",
    "\n",
    "def percentile(n):\n",
    "    def percentile_(x):\n",
    "        return np.percentile(x, n)\n",
    "    percentile_.__name__ = 'percentile_%s' % n\n",
    "    return percentile_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ba69b0",
   "metadata": {},
   "source": [
    "## Transaction related features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd8da806",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-2ac4872b09d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'op_device'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'net_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0mop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_null'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_null'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "# time related\n",
    "op['days'] = op['tm_diff'].apply(lambda x: int(x.split(' ')[0]))\n",
    "op['week'] = op['days'].apply(lambda x: x % 7)\n",
    "trans['days'] = trans['tm_diff'].apply(lambda x: int(x.split(' ')[0]))\n",
    "trans['week'] = trans['days'].apply(lambda x: x % 7)\n",
    "\n",
    "trans['days_diff'] = trans.groupby('user')['days'].diff()\n",
    "op['days_diff'] = op.groupby('user')['days'].diff()\n",
    "\n",
    "op['hour'] = op['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[0]))\n",
    "trans['hour'] = trans['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[0]))\n",
    "\n",
    "# money amount related\n",
    "temp = trans.groupby(['user'])['amount'].agg(\n",
    "        user_amount_mean = 'mean',\n",
    "        user_amount_std = 'std',\n",
    "        user_amount_max = 'max',\n",
    "        user_amount_min = 'min',\n",
    "        user_amount_sum = 'sum',\n",
    "        user_amount_med = 'median',\n",
    "        user_amount_cnt = 'count',\n",
    "        ).reset_index()\n",
    "\n",
    "data = pd.merge(data, temp, on = 'user', how = 'left')\n",
    "\n",
    "data['user_trans_days_nuniq'] = trans.groupby(['user'])['days'].nunique()\n",
    "\n",
    "\n",
    "data['user_amount_per_days'] = data['user_amount_sum'] / data['user_trans_days_nuniq']\n",
    "data = fill_na_zero(data, 'user_amount_per_days', 'user_trans_days_nuniq')\n",
    "data['user_amount_per_cnt'] = data['user_amount_sum'] / data['user_amount_cnt']\n",
    "data = fill_na_zero(data, 'user_amount_per_cnt', 'user_amount_cnt')\n",
    "\n",
    "\n",
    "\n",
    "for day in [7, 15, 23, 27]:\n",
    "    temp = group_data = trans[trans['days']>day].groupby('user')['amount'].agg(\n",
    "        col1='mean',\n",
    "        col2='std',\n",
    "        col3='max',\n",
    "        col4='min',\n",
    "        col5='sum',\n",
    "        col6='median',\n",
    "        col7='count',\n",
    "        ).reset_index().rename(columns = {'col1': 'user_amount_mean_{}d'.format(day), 'col2': 'user_amount_std_{}d'.format(day), \n",
    "        'col3':'user_amount_max_{}d'.format(day), 'col4': 'user_amount_min_{}d'.format(day), 'col5': 'user_amount_sum_{}d'.format(day), \n",
    "        'col6':'user_amount_med_{}d'.format(day), 'col7': 'user_amount_cnt_{}d'.format(day)})\n",
    "data = pd.merge(data, temp, on='user', how='left')\n",
    "\n",
    "\n",
    "win = [0, 15, 31]\n",
    "for i in range(len(win)-1):\n",
    "    temp = trans[(trans['days_diff']>=win[i])&(trans['days_diff']<win[i+1])].groupby('user')['amount'].agg(\n",
    "        col1='mean',\n",
    "        col2='std',\n",
    "        col3='max',\n",
    "        col4='min',\n",
    "        col5='sum',\n",
    "        col6='median',\n",
    "        col7='count',\n",
    "        ).reset_index().rename(columns = {'col1': 'user_amount_mean_{}_{}d'.format(win[i], win[i+1]), 'col2': 'user_amount_std_{}_{}d'.format(win[i], win[i+1]), \n",
    "        'col3':'user_amount_max_{}_{}d'.format(win[i], win[i+1]), 'col4': 'user_amount_min_{}_{}d'.format(win[i], win[i+1]), 'col5': 'user_amount_sum_{}_{}d'.format(win[i], win[i+1]), \n",
    "        'col6':'user_amount_med_{}_{}d'.format(win[i], win[i+1]), 'col7': 'user_amount_cnt_{}_{}d'.format(win[i], win[i+1])})\n",
    "data = pd.merge(data, temp, on = \"user\", how = 'left')\n",
    "\n",
    "\n",
    "# categorical variables\n",
    "for col in ['platform', 'tunnel_in', 'tunnel_out', 'type1', 'type2', 'ip', 'ip_3', 'days']:\n",
    "    temp = trans.groupby(['user'])[col].agg(col = 'nunique').reset_index().rename(columns = {'col': 'user_{}_nuniq_{}'.format(col, 'trans')})\n",
    "    data = data.merge(temp, on=['user'], how='left')\n",
    "\n",
    "for col in ['platform', 'type1', 'type2']:\n",
    "    temp = trans.pivot_table(index='user', columns=col, values='amount', dropna=False, aggfunc=['count', 'sum']).fillna(0)\n",
    "    temp.columns = ['user_{}_{}_amount_{}'.format(col, t[1], t[0]) for t in temp.columns]\n",
    "    temp = temp.reset_index()\n",
    "    data = pd.merge(data, temp, on='user', how='left')\n",
    "\n",
    "temp = data.groupby(['user'])['city'].agg(city_cnt = 'count').reset_index()\n",
    "data = pd.merge(data, temp, on = 'user', how = 'left')\n",
    "temp = data.groupby(['user'])['province'].agg(province_cnt = 'count').reset_index()\n",
    "data = pd.merge(data, temp, on = 'user', how = 'left')\n",
    "\n",
    "for col in [t for t in data.select_dtypes('object').columns if t not in ['user']]:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "\n",
    "# transaction fail ratio\n",
    "data['product7_fail_ratio'] = data['product7_fail_cnt'] / data['product7_cnt']\n",
    "data = fill_na_zero(data, 'product7_fail_ratio', 'product7_cnt')\n",
    "\n",
    "\n",
    "# transaction amount difference\n",
    "data['user_amount_gap'] = data['user_amount_max']-data['user_amount_min']\n",
    "\n",
    "data['user_trans_days_diff_nuniq'] = trans.groupby(['user'])['days_diff'].nunique()\n",
    "\n",
    "# average transaction times\n",
    "data['user_count_per_days'] = data['user_amount_cnt']/data['user_trans_days_diff_nuniq']\n",
    "data = fill_na_zero(data, 'user_count_per_days', 'user_trans_days_diff_nuniq')\n",
    "\n",
    "# average times of transaction platform, type1 and type2 used\n",
    "for value in ['platform', 'type1', 'type2']:\n",
    "    fea = list(set(trans[value]))\n",
    "    for f in fea:\n",
    "        if str(f)!= 'nan':\n",
    "            data['user_{}_{}_amount_perday_count'.format(value, f)] = data['user_{}_{}_amount_count'.format(value, f)]/data['user_trans_days_diff_nuniq']\n",
    "            data = fill_na_zero(data, 'user_{}_{}_amount_perday_count'.format(value, f), 'user_trans_days_diff_nuniq')\n",
    "\n",
    "# average transaction money amount of transaction platform, type1 and type2\n",
    "for value in ['platform', 'type1', 'type2']:\n",
    "    fea = list(set(trans[value]))\n",
    "    for f in fea:\n",
    "        if str(f)!= 'nan':\n",
    "            data['user_{}_{}_amount_perday'.format(value, f)] = data['user_{}_{}_amount_sum'.format(value, f)]/data['user_trans_days_diff_nuniq']\n",
    "            data = fill_na_zero(data, 'user_{}_{}_amount_perday'.format(value, f), 'user_trans_days_diff_nuniq')\n",
    "            data['user_{}_{}_amount_percount'.format(value, f)] = data['user_{}_{}_amount_sum'.format(value, f)]/data['user_amount_cnt']\n",
    "            data = fill_na_zero(data, 'user_{}_{}_amount_percount'.format(value, f), 'user_amount_cnt')\n",
    "\n",
    "# transaction gap days\n",
    "for value in ['platform', 'type1', 'type2']:\n",
    "    fea = list(set(trans[value]))\n",
    "    for f in fea:\n",
    "        if str(f)!= 'nan':\n",
    "            group_data = trans[trans[value]==f].groupby(['user'])['days_diff'].agg(col1 = 'min', col2 = 'max').reset_index().rename(columns = {'col1': 'user_{}_{}_min_day'.format(value, f), 'col2': 'user_{}_{}_max_day'.format(value, f)})\n",
    "            group_data['user_{}_{}_Is_recent_trd_mean'.format(value, f)]=(group_data['user_{}_{}_max_day'.format(value, f)]>group_data['user_{}_{}_max_day'.format(value, f)].mean())*1\n",
    "            group_data['user_{}_{}_gap_day'.format(value, f)] = group_data['user_{}_{}_max_day'.format(value, f)]-group_data['user_{}_{}_min_day'.format(value, f)]\n",
    "            data = data.merge(group_data, on=['user'], how='left')\n",
    "            #data = data.merge(group_data[['user','user_{}_{}_gap_day'.format(value, f)]], on=['user'], how='left')\n",
    "            #data['user_{}_{}_gap_day'.format(value, f)] = data['user_{}_{}_max_day'.format(value, f)]-data['user_{}_{}_min_day'.format(value, f)]\n",
    "\n",
    "\n",
    "\n",
    "# transaction gap money amount\n",
    "for value in ['platform', 'type1', 'type2']:\n",
    "    fea = list(set(trans[value]))\n",
    "    for f in fea:\n",
    "        if str(f)!= 'nan':\n",
    "            group_data = trans[trans[value]==f].groupby(['user'])['amount'].agg(col1 = 'min', col2 = 'max').reset_index().rename(columns = {'col1': 'user_{}_{}_min_amount'.format(value, f), 'col2':'user_{}_{}_max_amount'.format(value, f)})\n",
    "            data = data.merge(group_data, on=['user'], how='left')\n",
    "            data['user_{}_{}_gap_amount'.format(value, f)] = data['user_{}_{}_max_amount'.format(value, f)]-data['user_{}_{}_min_amount'.format(value, f)]\n",
    "\n",
    "recent_trd = trans.groupby(['user'])['days_diff'].agg(recent_trd = 'max').reset_index()\n",
    "\n",
    "recent_trd['Is_recent_trd_mean']=(recent_trd['recent_trd']>recent_trd['recent_trd'].mean())*1\n",
    "\n",
    "data = pd.merge(data, recent_trd, how='left', on='user')\n",
    "\n",
    "\n",
    "unique_col = list(set(data.columns))\n",
    "data = data[data.columns].T.drop_duplicates().T\n",
    "\n",
    "trans['hour_group'] = pd.cut(trans['hour'], [-1, 6, 12, 18, 24])\n",
    "\n",
    "# Time Period - User Habit Exploration\n",
    "for value in ['hour_group']:\n",
    "    fea = list(set(trans[value]))\n",
    "    for f in fea:\n",
    "        if str(f)!= 'nan':\n",
    "            temp = str(f).replace(\",\", \"_\")\n",
    "            temp = temp[1:-1]\n",
    "            group_data = trans[trans[value]==f].groupby(['user'])['amount'].agg(col1 = 'min', col2 = 'max', col3 = 'mean', col4 = 'count').reset_index().rename(columns = {'col1': 'user_{}_{}_min_amount'.format(value, temp), \n",
    "                               'col2':'user_{}_{}_max_amount'.format(value, temp), 'col3':'user_{}_{}_mean_amount'.format(value, temp), 'col4': 'user_{}_{}_cnt_amount'.format(value, temp)})\n",
    "            data = data.merge(group_data, on=['user'], how='left')\n",
    "            data['user_{}_{}_gap_amount'.format(value, temp)] = data['user_{}_{}_max_amount'.format(value, temp)]-data['user_{}_{}_min_amount'.format(value, temp)]\n",
    "\n",
    "# Statistical characteristics by week\n",
    "for value in ['week']:\n",
    "    fea = list(set(trans[value]))\n",
    "    for f in fea:\n",
    "        if str(f)!= 'nan':\n",
    "            group_data = trans[trans[value]==f].groupby(['user'])['amount'].agg(col1 = 'min', col2 = 'max', col3 = 'mean', col4 = 'count', col5 = 'sum', col6 = 'median', col7 = 'std', col8 = percentile(25), col9 = percentile(75)).reset_index().rename(columns = {'col1': 'user_{}_{}_min_amount'.format(value, f),\n",
    "                               'col2':'user_{}_{}_max_amount'.format(value, f), 'col3':'user_{}_{}_mean_amount'.format(value, f), 'col4': 'user_{}_{}_cnt_amount'.format(value, f),\n",
    "                               'col5':'user_{}_{}_sum_amount'.format(value, f), 'col6':'user_{}_{}_median_amount'.format(value, f), 'col7': 'user_{}_{}_std_amount'.format(value, f),\n",
    "                               'col8':'user_{}_{}_25_amount'.format(value, f), 'col9': 'user_{}_{}_75_amount'.format(value, f)})\n",
    "            #group_data['user_{}_{}_25_amount'.format(value, f)] = trans[trans[value]==f].groupby(['user']).agg(lambda x: np.percentile(x['amount'], q = 25))\n",
    "            data = data.merge(group_data, on=['user'], how='left')\n",
    "            data['user_{}_{}_gap_amount'.format(value, f)] = data['user_{}_{}_max_amount'.format(value, f)]-data['user_{}_{}_min_amount'.format(value, f)]\n",
    "\n",
    "# Number of trading days per week, average daily trade amount per week, number of trades\n",
    "for value in ['week']:\n",
    "    fea = list(set(trans[value]))\n",
    "    for f in fea:\n",
    "        if str(f)!= 'nan':\n",
    "            group_data = trans[trans[value]==f].groupby(['user'])['days_diff'].agg(col1 = 'nunique').reset_index().rename(columns = {'col1': 'user_{}_{}_nuniq_days'.format(value, f)})\n",
    "            data = data.merge(group_data, on=['user'], how='left')\n",
    "            data['user_{}_{}_amount_per_days'.format(value, f)] = data['user_{}_{}_sum_amount'.format(value, f)]/data['user_{}_{}_nuniq_days'.format(value, f)]\n",
    "            data = fill_na_zero(data, 'user_{}_{}_amount_per_days'.format(value, f), 'user_{}_{}_nuniq_days'.format(value, f))\n",
    "            data['user_{}_{}_times_per_days'.format(value, f)] = data['user_{}_{}_cnt_amount'.format(value, f)]/data['user_{}_{}_nuniq_days'.format(value, f)]\n",
    "            data = fill_na_zero(data, 'user_{}_{}_times_per_days'.format(value, f), 'user_{}_{}_nuniq_days'.format(value, f))\n",
    "\n",
    "for op_col in ['min', 'max', 'mean', 'cnt', 'sum', 'median', 'std', '25', '75', 'gap']:\n",
    "    col = []\n",
    "    for w in list(set(trans['week'])):\n",
    "        col.append('user_week_{}_{}_amount'.format(w, op_col))\n",
    "    temp = data[col]\n",
    "    data['user_allweek_{}_mean_amount'.format(op_col)] = np.nanmean(temp, axis = 1)\n",
    "    data['user_allweek_{}_std_amount'.format(op_col)] = np.nanstd(temp, axis = 1)\n",
    "\n",
    "\n",
    "for op_col in ['nuniq', 'amount_per', 'times_per']:\n",
    "    col = []\n",
    "    for w in list(set(trans['week'])):\n",
    "        col.append('user_week_{}_{}_days'.format(w, op_col))\n",
    "    temp = data[col]\n",
    "    data['user_allweek_{}_mean'.format(op_col)] = np.nanmean(temp, axis = 1)\n",
    "    data['user_allweek_{}_std'.format(op_col)] = np.nanstd(temp, axis = 1)\n",
    "\n",
    "# 25,75th percentile of the money amount\n",
    "group_data = trans.groupby(['user'])['amount'].agg(\n",
    "        user_amount_25 = percentile(25),\n",
    "        user_amount_75 = percentile(75)\n",
    "        ).reset_index()\n",
    "data = data.merge(group_data, on=['user'], how='left')\n",
    "\n",
    "\n",
    "# 25, 75th percentile of the amount of the time window\n",
    "for time_w in [7, 15, 23, 27]:\n",
    "    group_data = trans[trans['days_diff']>time_w].groupby('user')['amount'].agg(\n",
    "            col1 = percentile(25),\n",
    "            col2 = percentile(75)\n",
    "            ).reset_index().rename(columns = {'col1': 'user_amount_25_{}d'.format(time_w), 'col2': 'user_amount_75_{}d'.format(time_w)})\n",
    "    data = data.merge(group_data, on=['user'], how='left')\n",
    "\n",
    "for x in ['op_device', 'ip', 'net_type']:\n",
    "    op['is_null'] = 0\n",
    "    op.loc[op[x].isnull(), 'is_null'] = 1\n",
    "\n",
    "    temp = op.groupby(['user'])['is_null'].agg(\n",
    "            col1 = 'sum',\n",
    "            col2 = 'mean').reset_index().rename(columns = {'col1': 'user_{}_{}_null_cnt'.format('op', x), 'col2':'user_{}_{}_null_ratio'.format('op', x)})\n",
    "    data = pd.merge(data, temp, on = 'user', how = 'left')\n",
    "\n",
    "temp = op.groupby('user')['net_type'].agg(op_times = 'count').reset_index()\n",
    "data = pd.merge(data, temp, on=['user'], how='left')\n",
    "\n",
    "# net\n",
    "for x in list(set(op['net_type'])):\n",
    "    if str(temp) == 'nan':\n",
    "        continue\n",
    "    temp = op[op['net_type']==x].groupby('user')['net_type'].agg(col1 = 'count').reset_index().rename(columns = {'col1': 'user_{}_cnt'.format(x)})\n",
    "    data = pd.merge(data, temp, on=['user'], how='left')\n",
    "    data['user_{}_ratio'.format(x)] = data['user_{}_cnt'.format(x)]/data['op_times']\n",
    "\n",
    "# Aggregation degree\n",
    "def radio_d(x):\n",
    "    temp = Counter(x)\n",
    "    return max(list(temp.values()))/sum(temp.values())\n",
    "\n",
    "def radio_d_two(x):\n",
    "    temp = Counter(x)\n",
    "    if len(list(temp.values()))>=2:\n",
    "        return sum(sorted(list(temp.values()), reverse = True)[:2])/sum(temp.values())\n",
    "    else:\n",
    "        return max(list(temp.values()))/sum(temp.values())\n",
    "    \n",
    "def radio_d_three(x):\n",
    "    temp = Counter(x)\n",
    "    if len(list(temp.values()))>=3:\n",
    "        return sum(sorted(list(temp.values()), reverse = True)[:3])/sum(temp.values())\n",
    "    elif len(list(temp.values()))>=2:\n",
    "        return sum(sorted(list(temp.values()), reverse = True)[:2])/sum(temp.values())\n",
    "    else:\n",
    "        return max(list(temp.values()))/sum(temp.values())\n",
    "\n",
    "# user aggregation\n",
    "temp = op.groupby(['user'])['days_diff'].agg(\n",
    "            user_centre_1d = radio_d,\n",
    "            user_centre_2d = radio_d_two,\n",
    "            user_centre_3d = radio_d_three\n",
    "        ).reset_index()\n",
    "data = pd.merge(data, temp, on=['user'], how='left')\n",
    "\n",
    "\n",
    "'''\n",
    "temp = Counter(trans['days_diff'])\n",
    "tmp = max(list(temp.values()))/sum(temp.values())\n",
    "sum(sorted(list(temp.values()), reverse = True)[:2])/sum(temp.values())\n",
    "\n",
    "\n",
    "def radio_d(x):\n",
    "    temp = Counter(x)\n",
    "    #print(temp)\n",
    "    return max(list(temp.values()))/sum(temp.values())\n",
    "\n",
    "def radio_d_two(x):\n",
    "    temp = Counter(x)\n",
    "    if len(list(temp.values()))>=2:\n",
    "        return sum(sorted(list(temp.values()), reverse = True)[:2])/sum(temp.values())\n",
    "    else:\n",
    "        return max(list(temp.values()))/sum(temp.values())\n",
    "    \n",
    "def radio_d_three(x):\n",
    "    temp = Counter(x)\n",
    "    if len(list(temp.values()))>=3:\n",
    "        return sum(sorted(list(temp.values()), reverse = True)[:3])/sum(temp.values())\n",
    "    elif len(list(temp.values()))>=2:\n",
    "        return sum(sorted(list(temp.values()), reverse = True)[:2])/sum(temp.values())\n",
    "    else:\n",
    "        return max(list(temp.values()))/sum(temp.values())\n",
    "\n",
    "# User aggregation characteristics\n",
    "group_data = trans.groupby(['user'])['days_diff'].agg(\n",
    "            user_centre_1d = radio_d,\n",
    "            user_centre_2d = radio_d_two,\n",
    "            user_centre_3d = radio_d_three\n",
    "        ).reset_index()\n",
    "data = data.merge(group_data, on=['user'], how='left')\n",
    "\n",
    "''' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eab6b2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agreement1 0.9992905532293739\n",
      "product4_amount 0.9918344067773033\n",
      "product5_amount 0.9974126058953635\n",
      "user_trans_days_nuniq 1.0\n",
      "city_cnt 1.0\n",
      "user_platform_80f6d63f26a56315_min_day 0.9998469820690806\n",
      "user_platform_80f6d63f26a56315_max_day 0.9998469820690806\n",
      "user_platform_80f6d63f26a56315_Is_recent_trd_mean 0.999833071348088\n",
      "user_platform_fe8686492bb72dd4_min_day 0.9984141778068357\n",
      "user_platform_fe8686492bb72dd4_max_day 0.9984141778068357\n",
      "user_platform_fe8686492bb72dd4_Is_recent_trd_mean 0.9983446242018724\n",
      "user_platform_fe8686492bb72dd4_gap_day 0.9984141778068357\n",
      "user_type1_71b24e4fd9a658ee_min_day 0.9982611598759163\n",
      "user_type1_71b24e4fd9a658ee_max_day 0.9982611598759163\n",
      "user_type1_71b24e4fd9a658ee_Is_recent_trd_mean 0.9981776955499604\n",
      "user_type1_71b24e4fd9a658ee_gap_day 0.9982611598759163\n",
      "user_type1_c68ff01f3f65797d_min_day 0.9999721785580147\n",
      "user_type1_b26bc49195bd79cf_min_day 0.9986784815056965\n",
      "user_type1_b26bc49195bd79cf_max_day 0.9986784815056965\n",
      "user_type1_b26bc49195bd79cf_Is_recent_trd_mean 0.9986367493427184\n",
      "user_type1_b26bc49195bd79cf_gap_day 0.9986784815056965\n",
      "user_type1_fc9b75cf62ba8b8f_min_day 0.9995687676492273\n",
      "user_type1_fc9b75cf62ba8b8f_max_day 0.9995687676492273\n",
      "user_type1_fc9b75cf62ba8b8f_Is_recent_trd_mean 0.9995687676492273\n",
      "user_type1_fc9b75cf62ba8b8f_gap_day 0.9995687676492273\n",
      "user_type1_cd31880f9fa923ea_min_day 0.999749607022132\n",
      "user_type1_cd31880f9fa923ea_max_day 0.999749607022132\n",
      "user_type1_cd31880f9fa923ea_Is_recent_trd_mean 0.999749607022132\n",
      "user_type1_cd31880f9fa923ea_gap_day 0.999749607022132\n",
      "user_type1_81abaafd1ae512dd_min_day 0.9970509271495541\n",
      "user_type1_81abaafd1ae512dd_max_day 0.9970509271495541\n",
      "user_type1_81abaafd1ae512dd_Is_recent_trd_mean 0.9968979092186347\n",
      "user_type1_81abaafd1ae512dd_gap_day 0.9970509271495541\n",
      "user_type2_cd31880f9fa923ea_min_day 0.9965223197518327\n",
      "user_type2_cd31880f9fa923ea_max_day 0.9965223197518327\n",
      "user_type2_cd31880f9fa923ea_Is_recent_trd_mean 0.9963971232628988\n",
      "user_type2_cd31880f9fa923ea_gap_day 0.9965223197518327\n",
      "user_type2_08d5d580f39e34e6_min_day 0.9999443571160294\n",
      "user_type2_08d5d580f39e34e6_Is_recent_trd_mean 0.9999443571160294\n",
      "user_type2_08d5d580f39e34e6_gap_day 0.9999443571160294\n",
      "user_type2_81abaafd1ae512dd_min_day 0.9967309805667228\n",
      "user_type2_81abaafd1ae512dd_max_day 0.9967309805667228\n",
      "user_type2_81abaafd1ae512dd_Is_recent_trd_mean 0.9965779626358034\n",
      "user_type2_81abaafd1ae512dd_gap_day 0.9967309805667228\n",
      "user_type2_b26bc49195bd79cf_min_day 0.9988454101576084\n",
      "user_type2_b26bc49195bd79cf_max_day 0.9988454101576084\n",
      "user_type2_b26bc49195bd79cf_Is_recent_trd_mean 0.9988175887156231\n",
      "user_type2_b26bc49195bd79cf_gap_day 0.9988454101576084\n",
      "user_type2_2bf61669e40ef6b8_min_day 0.9988454101576084\n",
      "user_type2_2bf61669e40ef6b8_max_day 0.9988454101576084\n",
      "user_type2_2bf61669e40ef6b8_Is_recent_trd_mean 0.9986228386217257\n",
      "user_type2_2bf61669e40ef6b8_gap_day 0.9988454101576084\n",
      "user_type2_dde9095123fb6968_min_day 0.9999860892790073\n",
      "user_platform_80f6d63f26a56315_min_amount 0.999833071348088\n",
      "user_platform_80f6d63f26a56315_max_amount 0.999833071348088\n",
      "user_platform_80f6d63f26a56315_gap_amount 0.999833071348088\n",
      "user_platform_fe8686492bb72dd4_min_amount 0.9983446242018724\n",
      "user_platform_fe8686492bb72dd4_max_amount 0.9983446242018724\n",
      "user_platform_fe8686492bb72dd4_gap_amount 0.9983446242018724\n",
      "user_type1_71b24e4fd9a658ee_min_amount 0.9981776955499604\n",
      "user_type1_71b24e4fd9a658ee_max_amount 0.9981776955499604\n",
      "user_type1_71b24e4fd9a658ee_gap_amount 0.9981776955499604\n",
      "user_type1_c68ff01f3f65797d_min_amount 0.9999721785580147\n",
      "user_type1_b26bc49195bd79cf_min_amount 0.9986367493427184\n",
      "user_type1_b26bc49195bd79cf_max_amount 0.9986367493427184\n",
      "user_type1_b26bc49195bd79cf_gap_amount 0.9986367493427184\n",
      "user_type1_fc9b75cf62ba8b8f_min_amount 0.9995687676492273\n",
      "user_type1_fc9b75cf62ba8b8f_max_amount 0.9995687676492273\n",
      "user_type1_fc9b75cf62ba8b8f_gap_amount 0.9995687676492273\n",
      "user_type1_cd31880f9fa923ea_min_amount 0.999749607022132\n",
      "user_type1_cd31880f9fa923ea_max_amount 0.999749607022132\n",
      "user_type1_cd31880f9fa923ea_gap_amount 0.999749607022132\n",
      "user_type1_81abaafd1ae512dd_min_amount 0.9968979092186347\n",
      "user_type1_81abaafd1ae512dd_max_amount 0.9968979092186347\n",
      "user_type1_81abaafd1ae512dd_gap_amount 0.9968979092186347\n",
      "user_type2_cd31880f9fa923ea_min_amount 0.9963971232628988\n",
      "user_type2_cd31880f9fa923ea_max_amount 0.9963971232628988\n",
      "user_type2_cd31880f9fa923ea_gap_amount 0.9963971232628988\n",
      "user_type2_08d5d580f39e34e6_min_amount 0.9999443571160294\n",
      "user_type2_81abaafd1ae512dd_min_amount 0.9965779626358034\n",
      "user_type2_81abaafd1ae512dd_max_amount 0.9965779626358034\n",
      "user_type2_81abaafd1ae512dd_gap_amount 0.9965779626358034\n",
      "user_type2_b26bc49195bd79cf_min_amount 0.9988175887156231\n",
      "user_type2_b26bc49195bd79cf_max_amount 0.9988175887156231\n",
      "user_type2_b26bc49195bd79cf_gap_amount 0.9988175887156231\n",
      "user_type2_2bf61669e40ef6b8_min_amount 0.9986228386217257\n",
      "user_type2_2bf61669e40ef6b8_max_amount 0.9986228386217257\n",
      "user_type2_2bf61669e40ef6b8_gap_amount 0.9986228386217257\n",
      "user_type2_dde9095123fb6968_min_amount 0.9999860892790073\n",
      "user_nan_cnt 1.0\n",
      "user_nan_ratio 1.0\n"
     ]
    }
   ],
   "source": [
    "temp = op.groupby(['user'])['days_diff'].agg(\n",
    "            user_centre_1d = radio_d,\n",
    "            user_centre_2d = radio_d_two,\n",
    "            user_centre_3d = radio_d_three\n",
    "        ).reset_index()\n",
    "data = pd.merge(data, temp, on=['user'], how='left')\n",
    "\n",
    "good_cols = list(data.columns)\n",
    "\n",
    "for col in data.columns:\n",
    "    rate = data[col].value_counts(normalize=True, dropna=False).values[0]\n",
    "    if rate > 0.99:\n",
    "        good_cols.remove(col)\n",
    "        print(col, rate)\n",
    "\n",
    "data = data[good_cols]\n",
    "\n",
    "data.to_csv(\"Basic_feature.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ec3218",
   "metadata": {},
   "source": [
    "## Op related features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c0692adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_time(x):\n",
    "    day = int(x.split(' ')[0])\n",
    "    hour = int(x.split(' ')[2].split('.')[0].split(':')[0])\n",
    "    minute = int(x.split(' ')[2].split('.')[0].split(':')[1])\n",
    "    second = int(x.split(' ')[2].split('.')[0].split(':')[2])\n",
    "    return 86400*day+3600*hour+60*minute+second\n",
    "\n",
    "def generate_time(op):\n",
    "    op['days_diff'] = op['tm_diff'].apply(lambda x: int(x.split(' ')[0]))\n",
    "    op['timestamp'] = op['tm_diff'].apply(lambda x: cal_time(x))\n",
    "    op['hour'] = op['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[0]))\n",
    "    return op\n",
    "\n",
    "op=generate_time(op)\n",
    "trans=generate_time(trans)\n",
    "\n",
    "\n",
    "op['index']=op.index   \n",
    "# user operation times\n",
    "table = pd.pivot_table(op, values=['index'], index=['user'], aggfunc={pd.Series.nunique})  \n",
    "table.columns = [' '.join(col).strip() for col in table.columns.values]  \n",
    "table=table.reset_index()   \n",
    "data=data[['user','label']].copy()  \n",
    "table=table.merge(data[['user','label']],how='left',on=['user'])  \n",
    "op=op.drop(columns=['index'])  \n",
    "table=table.rename(columns={'index nunique':'op_times'})  \n",
    "data=data.merge(table[['user','op_times']],how='left',on=['user'])  \n",
    "data['op_times']=data['op_times'].fillna(0)  \n",
    "data.dropna()  \n",
    "trans_data=pd.read_csv('train_trans.csv')\n",
    "trans_test=pd.read_csv('testb_trans.csv')  \n",
    "trans=pd.concat([trans_data,trans_test],axis=0)   \n",
    "# user transaction times\n",
    "trans['index']=trans.index\n",
    "table2 = pd.pivot_table(trans, values=['index'], index=['user'], aggfunc={pd.Series.nunique})  \n",
    "table2.columns = [' '.join(col).strip() for col in table2.columns.values]\n",
    "table2=table2.reset_index()  \n",
    "table2=table2.rename(columns={'index nunique':'trans_times'})  \n",
    "data=data.merge(table2[['user','trans_times']],how='left',on='user')  \n",
    "data.dropna()  \n",
    "data['trans_times']=data['trans_times'].fillna(0)  \n",
    "data['op_per_trans']=data['op_times']/data['trans_times']  \n",
    "data['op_per_trans']=data['op_per_trans'].fillna(0)  \n",
    "data.dropna()   \n",
    "op=generate_time(op)  \n",
    "op=op.sort_values(by=['user','timestamp']) \n",
    "\n",
    " #################### days which have operation records##########\n",
    "table = pd.pivot_table(op, values=['days_diff'], index=['user'], aggfunc={pd.Series.nunique})  \n",
    "table.columns = [' '.join(col).strip() for col in table.columns.values]  \n",
    "table=table.reset_index()  \n",
    "table=table.merge(data[['user','label']],how='left',on=['user'])  \n",
    "data=data.merge(table[['user','days_diff nunique']],how='left',on=['user'])  \n",
    "data['days_diff nunique']=data['days_diff nunique'].fillna(0)  \n",
    "\n",
    "########### average operation times #########\n",
    "data.dropna()  \n",
    "data['op_per_day']=data['trans_times']/data['days_diff nunique']  \n",
    "data['op_per_day']=data['op_per_day'].fillna(0)  \n",
    "\n",
    "########### user operation days related features ########\n",
    "op['days_diff'].astype(int)\n",
    "op_days_unique=op.drop_duplicates(subset=['user','days_diff'])  \n",
    "tmp = op_days_unique.groupby('user', as_index=False)['days_diff'].agg({list})  \n",
    "tmp=tmp.reset_index()  \n",
    "tmp=tmp.merge(data[['user','label']],how='left',on=['user'])   \n",
    "\n",
    "def mean_interval(nums):\n",
    "    sum_num=0\n",
    "    for i in range(len(nums)-1):\n",
    "        sum_num+=nums[i+1]-nums[i]\n",
    "    if len(nums)==1:\n",
    "        return 0\n",
    "    return sum_num/(len(nums)-1) \n",
    "tmp['mean_interval']=tmp['list'].apply(lambda x:mean_interval(x))  \n",
    "\n",
    "tmp['deviation']=tmp['list'].apply(lambda x:np.std(x))   \n",
    "\n",
    "tmp['mean_value']=tmp['list'].apply(lambda x:np.mean(x))   \n",
    "\n",
    "def max_interval(nums):\n",
    "    max_interval=1\n",
    "    for i in range(len(nums)-1):\n",
    "        max_interval=max(max_interval,nums[i+1]-nums[i])\n",
    "    return max_interval \n",
    "tmp['max_interval']=tmp['list'].apply(lambda x:max_interval(x))  \n",
    "\n",
    "############# The time difference between the date of the last operation and the last day\n",
    "tmp['last_op_day']=tmp['list'].apply(lambda x:np.max(x))  \n",
    "data=data.merge(tmp[['user','mean_interval', 'deviation', 'mean_value',\n",
    "       'max_interval']],how='left',on=['user'])  \n",
    "data=data.merge(tmp[['user','last_op_day']],how='left',on=['user'])  \n",
    "data['mean_interval']=data['mean_interval'].fillna(15)\n",
    "data['max_interval']=data['max_interval'].fillna(15)\n",
    "data['deviation']=data['deviation'].fillna(0)\n",
    "data['last_op_day']=data['last_op_day'].fillna(0)  \n",
    "data['mean_value']=data['mean_value'].fillna(0)\n",
    "data.dropna()   \n",
    "\n",
    "################the number of missing records for ip accounts for the total number of records\n",
    "op['ip_3']=op['ip_3'].fillna(-1)  \n",
    "op_na=op[op['ip_3']==-1]  \n",
    "op_na['index']=op_na.index\n",
    "table = pd.pivot_table(op_na, values=['index'], index=['user'], aggfunc={pd.Series.nunique})  \n",
    "table.columns = [' '.join(col).strip() for col in table.columns.values]  \n",
    "table=table.reset_index()  \n",
    "table=table.merge(data[['user','label']],how='right',on=['user'])  \n",
    "table['index nunique']=table['index nunique'].fillna(0)  \n",
    "table=table.rename(columns={'index nunique':'na_ip_times'})  \n",
    "data=data.merge(table[['user','na_ip_times']],how='left',on=['user'])  \n",
    "data['na_ip_times']=data['na_ip_times'].fillna(0)  \n",
    "data['nan_ip_percent']=data['na_ip_times']/data['na_ip_times']  \n",
    "data['nan_ip_percent']=data['nan_ip_percent'].fillna(0)   \n",
    "# Strong feature gain\n",
    "data['mean_interval*deviation']=data['mean_interval']*data['deviation']  \n",
    "data['mean_interval*nan_ip_per']=data['mean_interval']*data['nan_ip_percent'] \n",
    "\n",
    "# proportion of each operation type to the total number of operations\n",
    "op['index']=op.index\n",
    "table = pd.pivot_table(op[op['op_type']=='af1d68e0408f5148'], values=['index'], index=['user'], aggfunc={pd.Series.nunique})  \n",
    "table.columns = [' '.join(col).strip() for col in table.columns.values]  \n",
    "table=table.reset_index()  \n",
    "table=table.rename(columns={'index nunique':'op_type1_times'})\n",
    "  \n",
    "data=data.merge(table,how='left',on='user')  \n",
    "data['op_type1_times']=data['op_type1_times'].fillna(0)  \n",
    "data['op_type1_times_percent']=data['op_type1_times']/data['op_times']  \n",
    "data['op_type1_times_percent']=data['op_type1_times_percent'].fillna(0)  \n",
    "op['index']=op.index\n",
    "table = pd.pivot_table(op[op['op_type']=='b26bc49195bd79cf'], values=['index'], index=['user'], aggfunc={pd.Series.nunique})  \n",
    "table.columns = [' '.join(col).strip() for col in table.columns.values]  \n",
    "table=table.reset_index()  \n",
    "table=table.rename(columns={'index nunique':'op_type2_times'})  \n",
    "data=data.merge(table,how='left',on='user')  \n",
    "data['op_type2_times']=data['op_type2_times'].fillna(0)  \n",
    "data['op_type2_times_percent']=data['op_type2_times']/data['op_times']  \n",
    "data['op_type2_times_percent']=data['op_type2_times_percent'].fillna(0)  \n",
    "data.dropna()  \n",
    "\n",
    "data['is_inf']=data.apply(lambda row:math.isinf(row['op_per_day']),axis=1)  \n",
    "data.loc[data['is_inf']==True,'op_per_day']=0  \n",
    "data=data.drop(columns=['is_inf'])  \n",
    "\n",
    "data=data.drop(columns=['days_diff','timestamp','hour'])\n",
    "\n",
    "data['sum_card_number']=data['card_a_cnt']+data['card_b_cnt']+data['card_c_cnt']+data['card_d_cnt']\n",
    "\n",
    "data['card_a_percent']=data['card_a_cnt']/data['sum_card_number']\n",
    "data['card_b_percent']=data['card_b_cnt']/data['sum_card_number']\n",
    "data['card_c_percent']=data['card_c_cnt']/data['sum_card_number']\n",
    "data['card_d_percent']=data['card_d_cnt']/data['sum_card_number']\n",
    "\n",
    "data['card_a_percent']=data['card_a_percent'].fillna(0)\n",
    "data['card_b_percent']=data['card_b_percent'].fillna(0)\n",
    "data['card_c_percent']=data['card_c_percent'].fillna(0)\n",
    "data['card_d_percent']=data['card_d_percent'].fillna(0)\n",
    "\n",
    "data['op_count']=data['op1_cnt']+data['op2_cnt']\n",
    "data['op1_percent']=data['op1_cnt']/data['op_count']\n",
    "data['op2_percent']=data['op2_cnt']/data['op_count']\n",
    "data['op1_percent']=data['op1_percent'].fillna(0)\n",
    "data['op2_percent']=data['op2_percent'].fillna(0)\n",
    "\n",
    "data['service_cnt']=data['service1_cnt']+data['service2_cnt']\n",
    "data['service1_percent']=data['service1_cnt']/data['service_cnt']\n",
    "data['service2_percent']=data['service2_cnt']/data['service_cnt']\n",
    "data['service1_percent']=data['service1_percent'].fillna(0)\n",
    "data['service2_percent']=data['service2_percent'].fillna(0)\n",
    "\n",
    "# Percentage of different types of balance grades\n",
    "data['balance1_avg_per']=data['balance1_avg']/data['balance_avg']\n",
    "data['balance2_avg_per']=data['balance2_avg']/data['balance_avg']\n",
    "\n",
    "# percentage of different products with different amount levels\n",
    "data['product_amount']=data['product1_amount']+data['product2_amount']+data['product3_amount']+data['product4_amount']+data['product5_amount']+data['product6_amount']\n",
    "for i in range(1,7):\n",
    "    column_name='product'+str(i)+'_amount'\n",
    "    data[column_name+'_percent']=data[column_name]/data['product_amount']\n",
    "    data[column_name+'_percent']=data[column_name+'_percent'].fillna(0)\n",
    "\n",
    "\n",
    "data['login_cnt_period_std']=data.apply(lambda row:np.std([row['login_cnt_period1'],row['login_cnt_period2']]),axis=1)\n",
    "\n",
    "data['login_period1_percent']=data['login_cnt_period1']/(data['login_cnt_period1']+data['login_cnt_period2'])\n",
    "data['login_period2_percent']=data['login_cnt_period2']/(data['login_cnt_period1']+data['login_cnt_period2'])\n",
    "data['login_period1_percent']=data['login_period1_percent'].fillna(0)\n",
    "data['login_period2_percent']=data['login_period2_percent'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2b554377",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Basic_feature.csv')\n",
    "\n",
    "col1 = list(df.columns)\n",
    "col2 = list(data.columns)\n",
    "column = [x for x in col2 if x not in col1]\n",
    "data = data[column+['user']]\n",
    "\n",
    "data = pd.merge(data, df, on = 'user', how = 'right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a97066d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_platform_80f6d63f26a56315_amount_count 0.999833071348088\n",
      "user_platform_fe8686492bb72dd4_amount_count 0.9983446242018724\n",
      "user_platform_80f6d63f26a56315_amount_sum 0.999833071348088\n",
      "user_platform_fe8686492bb72dd4_amount_sum 0.9983446242018724\n",
      "user_type1_71b24e4fd9a658ee_amount_count 0.9981776955499604\n",
      "user_type1_81abaafd1ae512dd_amount_count 0.9968979092186347\n",
      "user_type1_b26bc49195bd79cf_amount_count 0.9986367493427184\n",
      "user_type1_c68ff01f3f65797d_amount_count 0.9999721785580147\n",
      "user_type1_cd31880f9fa923ea_amount_count 0.999749607022132\n",
      "user_type1_fc9b75cf62ba8b8f_amount_count 0.9995687676492273\n",
      "user_type1_71b24e4fd9a658ee_amount_sum 0.9981776955499604\n",
      "user_type1_81abaafd1ae512dd_amount_sum 0.9968979092186347\n",
      "user_type1_b26bc49195bd79cf_amount_sum 0.9986367493427184\n",
      "user_type1_c68ff01f3f65797d_amount_sum 0.9999721785580147\n",
      "user_type1_cd31880f9fa923ea_amount_sum 0.999749607022132\n",
      "user_type1_fc9b75cf62ba8b8f_amount_sum 0.9995687676492273\n",
      "user_type2_08d5d580f39e34e6_amount_count 0.9999443571160294\n",
      "user_type2_2bf61669e40ef6b8_amount_count 0.9986228386217257\n",
      "user_type2_81abaafd1ae512dd_amount_count 0.9965779626358034\n",
      "user_type2_b26bc49195bd79cf_amount_count 0.9988175887156231\n",
      "user_type2_cd31880f9fa923ea_amount_count 0.9963971232628988\n",
      "user_type2_dde9095123fb6968_amount_count 0.9999860892790073\n",
      "user_type2_08d5d580f39e34e6_amount_sum 0.9999443571160294\n",
      "user_type2_2bf61669e40ef6b8_amount_sum 0.9986228386217257\n",
      "user_type2_81abaafd1ae512dd_amount_sum 0.9965779626358034\n",
      "user_type2_b26bc49195bd79cf_amount_sum 0.9988175887156231\n",
      "user_type2_cd31880f9fa923ea_amount_sum 0.9963971232628988\n",
      "user_type2_dde9095123fb6968_amount_sum 0.9999860892790073\n",
      "user_platform_80f6d63f26a56315_amount_percount 0.999833071348088\n",
      "user_platform_fe8686492bb72dd4_amount_percount 0.9983446242018724\n",
      "user_type1_71b24e4fd9a658ee_amount_percount 0.9981776955499604\n",
      "user_type1_c68ff01f3f65797d_amount_percount 0.9999721785580147\n",
      "user_type1_b26bc49195bd79cf_amount_percount 0.9986367493427184\n",
      "user_type1_fc9b75cf62ba8b8f_amount_percount 0.9995687676492273\n",
      "user_type1_cd31880f9fa923ea_amount_percount 0.999749607022132\n",
      "user_type1_81abaafd1ae512dd_amount_percount 0.9968979092186347\n",
      "user_type2_cd31880f9fa923ea_amount_percount 0.9963971232628988\n",
      "user_type2_08d5d580f39e34e6_amount_percount 0.9999443571160294\n",
      "user_type2_81abaafd1ae512dd_amount_percount 0.9965779626358034\n",
      "user_type2_b26bc49195bd79cf_amount_percount 0.9988175887156231\n",
      "user_type2_2bf61669e40ef6b8_amount_percount 0.9986228386217257\n",
      "user_type2_dde9095123fb6968_amount_percount 0.9999860892790073\n",
      "user_platform_cc79bc9b7f4885de_Is_recent_trd_mean 0.9903598703520804\n",
      "user_type1_2a2edd435db5ac70_Is_recent_trd_mean 0.9902624953051317\n",
      "user_type1_dde9095123fb6968_min_day 0.990081655932227\n",
      "user_type1_dde9095123fb6968_Is_recent_trd_mean 0.9938097291582623\n",
      "user_type1_dde9095123fb6968_gap_day 0.9949921404426392\n",
      "user_type1_e5cd41dfa47665b1_Is_recent_trd_mean 0.99239083561701\n",
      "user_type1_e5cd41dfa47665b1_gap_day 0.9918761389402813\n",
      "user_type1_8adb3dcfea9dcf5e_Is_recent_trd_mean 0.990665906213919\n",
      "user_type1_fc4eca960f6c690c_min_day 0.9925438535479294\n",
      "user_type1_fc4eca960f6c690c_max_day 0.9927525143628194\n",
      "user_type1_fc4eca960f6c690c_Is_recent_trd_mean 0.9952842655834852\n",
      "user_type1_fc4eca960f6c690c_gap_day 0.9955068371193679\n",
      "user_type1_0f99a10a1331ce14_Is_recent_trd_mean 0.9935454254594016\n",
      "user_type1_0f99a10a1331ce14_gap_day 0.9952981763044778\n",
      "user_type2_2dd805cd09533f85_Is_recent_trd_mean 0.9924603892219733\n",
      "user_type2_2dd805cd09533f85_gap_day 0.9961884624480086\n",
      "user_type2_2e774c57ed375d33_Is_recent_trd_mean 0.9915979245204279\n",
      "user_type1_dde9095123fb6968_gap_amount 0.993907104205211\n",
      "user_type1_e5cd41dfa47665b1_min_amount 0.992223906965098\n",
      "user_type1_e5cd41dfa47665b1_gap_amount 0.9938514613212402\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trans_times</th>\n",
       "      <th>op_per_trans</th>\n",
       "      <th>days_diff nunique</th>\n",
       "      <th>op_per_day</th>\n",
       "      <th>mean_interval</th>\n",
       "      <th>deviation</th>\n",
       "      <th>mean_value</th>\n",
       "      <th>max_interval</th>\n",
       "      <th>last_op_day</th>\n",
       "      <th>na_ip_times</th>\n",
       "      <th>...</th>\n",
       "      <th>op_times</th>\n",
       "      <th>user_b2e7fa260df4998d_cnt</th>\n",
       "      <th>user_b2e7fa260df4998d_ratio</th>\n",
       "      <th>user_116a2503b987ea81_cnt</th>\n",
       "      <th>user_116a2503b987ea81_ratio</th>\n",
       "      <th>user_b131ac74aa38a121_cnt</th>\n",
       "      <th>user_b131ac74aa38a121_ratio</th>\n",
       "      <th>user_centre_1d</th>\n",
       "      <th>user_centre_2d</th>\n",
       "      <th>user_centre_3d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>12.333333</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.952381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1.555556</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.0</td>\n",
       "      <td>3.222222</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.424674</td>\n",
       "      <td>4.777778</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.477273</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.810345</td>\n",
       "      <td>0.844828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 480 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   trans_times  op_per_trans  days_diff nunique  op_per_day  mean_interval  \\\n",
       "0          4.0     10.500000                3.0    1.333333           1.50   \n",
       "1          2.0      1.500000                1.0    2.000000           0.00   \n",
       "2          9.0      1.555556                1.0    9.000000           0.00   \n",
       "3         18.0      3.222222                9.0    2.000000           1.25   \n",
       "4          1.0     11.000000                2.0    0.500000           3.00   \n",
       "\n",
       "   deviation  mean_value  max_interval  last_op_day  na_ip_times  ...  \\\n",
       "0   1.247219   12.333333           2.0         14.0          7.0  ...   \n",
       "1   0.000000   11.000000           1.0         11.0          0.0  ...   \n",
       "2   0.000000    6.000000           1.0          6.0          4.0  ...   \n",
       "3   3.424674    4.777778           2.0         10.0         19.0  ...   \n",
       "4   1.500000   11.500000           3.0         13.0          1.0  ...   \n",
       "\n",
       "   op_times  user_b2e7fa260df4998d_cnt  user_b2e7fa260df4998d_ratio  \\\n",
       "0      12.0                        NaN                          NaN   \n",
       "1       3.0                        NaN                          NaN   \n",
       "2       5.0                        1.0                     0.200000   \n",
       "3      44.0                        1.0                     0.022727   \n",
       "4       0.0                        NaN                          NaN   \n",
       "\n",
       "   user_116a2503b987ea81_cnt  user_116a2503b987ea81_ratio  \\\n",
       "0                       12.0                     1.000000   \n",
       "1                        3.0                     1.000000   \n",
       "2                        NaN                          NaN   \n",
       "3                       21.0                     0.477273   \n",
       "4                        NaN                          NaN   \n",
       "\n",
       "   user_b131ac74aa38a121_cnt  user_b131ac74aa38a121_ratio user_centre_1d  \\\n",
       "0                        NaN                          NaN       0.904762   \n",
       "1                        NaN                          NaN       0.666667   \n",
       "2                        4.0                          0.8       0.928571   \n",
       "3                       22.0                          0.5       0.775862   \n",
       "4                        NaN                          NaN       0.818182   \n",
       "\n",
       "   user_centre_2d  user_centre_3d  \n",
       "0        0.928571        0.952381  \n",
       "1        1.000000        1.000000  \n",
       "2        1.000000        1.000000  \n",
       "3        0.810345        0.844828  \n",
       "4        0.909091        1.000000  \n",
       "\n",
       "[5 rows x 480 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_cols = list(data.columns)\n",
    "\n",
    "for col in data.columns:\n",
    "    rate = data[col].value_counts(normalize=True, dropna=False).values[0]\n",
    "    if rate > 0.99:\n",
    "        good_cols.remove(col)\n",
    "        print(col, rate)\n",
    "\n",
    "data = data[good_cols]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c43b6bc",
   "metadata": {},
   "source": [
    "Word2Vec for amount, op_type...(text) feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fe983cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================== user amount ======================================\n",
      "====================================== user op_type ======================================\n",
      "====================================== user op_mode ======================================\n",
      "====================================== user op_device ======================================\n",
      "====================================== user ip ======================================\n",
      "====================================== user net_type ======================================\n",
      "====================================== user channel ======================================\n",
      "====================================== user ip_3 ======================================\n"
     ]
    }
   ],
   "source": [
    "def emb(df, f1, f2):\n",
    "    # size represents the dimensionality of the output feature vector, which is generally taken to be between a few tens and a few hundred\n",
    "    emb_size = 64\n",
    "    print('====================================== {} {} ======================================'.format(f1, f2))\n",
    "\n",
    "    df[f2]=df[f2].fillna('0')\n",
    "    \n",
    "    # Data sorting, first keyword user, second keyword timestamp\n",
    "    df=df.sort_values(by=['user','timestamp'])\n",
    "    tmp = df.groupby(f1, as_index=False)[f2].agg({'{}_{}_list'.format(f1, f2): list})\n",
    "    sentences = tmp['{}_{}_list'.format(f1, f2)].values.tolist()\n",
    "    del tmp['{}_{}_list'.format(f1, f2)]\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = [str(x) for x in sentences[i]]\n",
    "    model = Word2Vec(sentences, vector_size=emb_size, window=5, min_count=3, sg=0, hs=1, seed=2222)\n",
    "    emb_matrix = []\n",
    "    for seq in sentences:\n",
    "        vec = []\n",
    "        for w in seq:\n",
    "            if w in model.wv:\n",
    "                vec.append(model.wv[w])\n",
    "        if len(vec) > 0:\n",
    "            emb_matrix.append(np.mean(vec, axis=0))\n",
    "        else:\n",
    "            emb_matrix.append([0] * emb_size)\n",
    "    emb_matrix = np.array(emb_matrix)\n",
    "    for i in range(emb_size):\n",
    "        tmp['{}_{}_emb_{}'.format(f1, f2, i)] = emb_matrix[:, i]\n",
    "    del model, emb_matrix, sentences\n",
    "    return tmp\n",
    "\n",
    "# Transaction Amount\n",
    "\n",
    "emb_cols = [\n",
    "    ['user', 'amount']\n",
    "]\n",
    "\n",
    "amount_w2v=pd.DataFrame(data['user'].copy())\n",
    "\n",
    "trans=generate_time(trans)\n",
    "\n",
    "for f1, f2 in emb_cols:\n",
    "    amount_w2v = amount_w2v.merge(emb(trans, f1, f2), on=f1, how='left')\n",
    "\n",
    "#%%\n",
    "amount_w2v=amount_w2v.drop(columns=['user'])\n",
    "amount_w2v['sum_op_w2v']=amount_w2v.sum(axis=1)\n",
    "\n",
    "data=pd.concat([data,amount_w2v['sum_op_w2v']],axis=1)\n",
    "\n",
    "#%% operation data\n",
    "emb_cols = [\n",
    "    ['user','op_type'],\n",
    "    ['user','op_mode'],\n",
    "    ['user','op_device'],\n",
    "    ['user','ip'],\n",
    "    ['user','net_type'],\n",
    "    ['user','channel'],\n",
    "    ['user','ip_3'],\n",
    "]\n",
    "\n",
    "op=generate_time(op)\n",
    "\n",
    "op_w2v=pd.DataFrame(data['user'].copy())\n",
    "\n",
    "for f1, f2 in emb_cols:\n",
    "    op_w2v=emb(op, f1, f2)\n",
    "    op_w2v=op_w2v.drop(columns=['user'])\n",
    "    column_name='sum_w2v'+f2\n",
    "    op_w2v[column_name]=op_w2v.sum(axis=1)\n",
    "    data=pd.concat([data,op_w2v[column_name]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "38256bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trans_times</th>\n",
       "      <th>op_per_trans</th>\n",
       "      <th>days_diff nunique</th>\n",
       "      <th>op_per_day</th>\n",
       "      <th>mean_interval</th>\n",
       "      <th>deviation</th>\n",
       "      <th>mean_value</th>\n",
       "      <th>max_interval</th>\n",
       "      <th>last_op_day</th>\n",
       "      <th>na_ip_times</th>\n",
       "      <th>...</th>\n",
       "      <th>user_centre_2d</th>\n",
       "      <th>user_centre_3d</th>\n",
       "      <th>sum_op_w2v</th>\n",
       "      <th>sum_w2vop_type</th>\n",
       "      <th>sum_w2vop_mode</th>\n",
       "      <th>sum_w2vop_device</th>\n",
       "      <th>sum_w2vip</th>\n",
       "      <th>sum_w2vnet_type</th>\n",
       "      <th>sum_w2vchannel</th>\n",
       "      <th>sum_w2vip_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>12.333333</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>-4.637256</td>\n",
       "      <td>-8.136339</td>\n",
       "      <td>-0.475300</td>\n",
       "      <td>2.985004</td>\n",
       "      <td>-5.030328</td>\n",
       "      <td>4.763794</td>\n",
       "      <td>7.440312</td>\n",
       "      <td>-2.014670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.111074</td>\n",
       "      <td>-5.972022</td>\n",
       "      <td>-0.836419</td>\n",
       "      <td>1.776785</td>\n",
       "      <td>0.502700</td>\n",
       "      <td>5.272960</td>\n",
       "      <td>6.264074</td>\n",
       "      <td>5.748219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1.555556</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.832696</td>\n",
       "      <td>-1.755416</td>\n",
       "      <td>7.076905</td>\n",
       "      <td>9.253852</td>\n",
       "      <td>3.914774</td>\n",
       "      <td>5.354793</td>\n",
       "      <td>6.183161</td>\n",
       "      <td>2.786482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.0</td>\n",
       "      <td>3.222222</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.424674</td>\n",
       "      <td>4.777778</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.810345</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0.516070</td>\n",
       "      <td>-6.728838</td>\n",
       "      <td>-0.591093</td>\n",
       "      <td>4.799207</td>\n",
       "      <td>4.165147</td>\n",
       "      <td>3.778757</td>\n",
       "      <td>1.146504</td>\n",
       "      <td>-10.967967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.320182</td>\n",
       "      <td>-1.100410</td>\n",
       "      <td>-0.557338</td>\n",
       "      <td>1.752873</td>\n",
       "      <td>-1.629506</td>\n",
       "      <td>-0.095596</td>\n",
       "      <td>-0.823160</td>\n",
       "      <td>8.272836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 488 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   trans_times  op_per_trans  days_diff nunique  op_per_day  mean_interval  \\\n",
       "0          4.0     10.500000                3.0    1.333333           1.50   \n",
       "1          2.0      1.500000                1.0    2.000000           0.00   \n",
       "2          9.0      1.555556                1.0    9.000000           0.00   \n",
       "3         18.0      3.222222                9.0    2.000000           1.25   \n",
       "4          1.0     11.000000                2.0    0.500000           3.00   \n",
       "\n",
       "   deviation  mean_value  max_interval  last_op_day  na_ip_times  ...  \\\n",
       "0   1.247219   12.333333           2.0         14.0          7.0  ...   \n",
       "1   0.000000   11.000000           1.0         11.0          0.0  ...   \n",
       "2   0.000000    6.000000           1.0          6.0          4.0  ...   \n",
       "3   3.424674    4.777778           2.0         10.0         19.0  ...   \n",
       "4   1.500000   11.500000           3.0         13.0          1.0  ...   \n",
       "\n",
       "   user_centre_2d  user_centre_3d  sum_op_w2v  sum_w2vop_type  sum_w2vop_mode  \\\n",
       "0        0.928571        0.952381   -4.637256       -8.136339       -0.475300   \n",
       "1        1.000000        1.000000    0.111074       -5.972022       -0.836419   \n",
       "2        1.000000        1.000000   -2.832696       -1.755416        7.076905   \n",
       "3        0.810345        0.844828    0.516070       -6.728838       -0.591093   \n",
       "4        0.909091        1.000000    0.320182       -1.100410       -0.557338   \n",
       "\n",
       "   sum_w2vop_device  sum_w2vip sum_w2vnet_type  sum_w2vchannel  sum_w2vip_3  \n",
       "0          2.985004  -5.030328        4.763794        7.440312    -2.014670  \n",
       "1          1.776785   0.502700        5.272960        6.264074     5.748219  \n",
       "2          9.253852   3.914774        5.354793        6.183161     2.786482  \n",
       "3          4.799207   4.165147        3.778757        1.146504   -10.967967  \n",
       "4          1.752873  -1.629506       -0.095596       -0.823160     8.272836  \n",
       "\n",
       "[5 rows x 488 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "78df89b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = fill_discrete_conti_na(data)\n",
    "data.to_csv(\"data_feature_final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc36c5d6",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e4431939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current num of features: 486\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[100]\tvalid_0's auc: 0.698224\n",
      "[200]\tvalid_0's auc: 0.704796\n",
      "[300]\tvalid_0's auc: 0.708892\n",
      "[400]\tvalid_0's auc: 0.711597\n",
      "[500]\tvalid_0's auc: 0.713495\n",
      "[600]\tvalid_0's auc: 0.714659\n",
      "[700]\tvalid_0's auc: 0.715557\n",
      "[800]\tvalid_0's auc: 0.716263\n",
      "[900]\tvalid_0's auc: 0.716706\n",
      "[1000]\tvalid_0's auc: 0.716935\n",
      "[1100]\tvalid_0's auc: 0.717368\n",
      "[1200]\tvalid_0's auc: 0.717466\n",
      "[1300]\tvalid_0's auc: 0.717659\n",
      "[1400]\tvalid_0's auc: 0.717611\n",
      "Early stopping, best iteration is:\n",
      "[1299]\tvalid_0's auc: 0.71767\n",
      "[0.7176699661360936]\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[100]\tvalid_0's auc: 0.713842\n",
      "[200]\tvalid_0's auc: 0.72262\n",
      "[300]\tvalid_0's auc: 0.72691\n",
      "[400]\tvalid_0's auc: 0.729911\n",
      "[500]\tvalid_0's auc: 0.731822\n",
      "[600]\tvalid_0's auc: 0.733348\n",
      "[700]\tvalid_0's auc: 0.734141\n",
      "[800]\tvalid_0's auc: 0.73454\n",
      "[900]\tvalid_0's auc: 0.734698\n",
      "[1000]\tvalid_0's auc: 0.734587\n",
      "Early stopping, best iteration is:\n",
      "[922]\tvalid_0's auc: 0.734778\n",
      "[0.7176699661360936, 0.7347776872152024]\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[100]\tvalid_0's auc: 0.710459\n",
      "[200]\tvalid_0's auc: 0.717442\n",
      "[300]\tvalid_0's auc: 0.720754\n",
      "[400]\tvalid_0's auc: 0.722642\n",
      "[500]\tvalid_0's auc: 0.723915\n",
      "[600]\tvalid_0's auc: 0.72441\n",
      "[700]\tvalid_0's auc: 0.724766\n",
      "[800]\tvalid_0's auc: 0.725063\n",
      "[900]\tvalid_0's auc: 0.725322\n",
      "[1000]\tvalid_0's auc: 0.725297\n",
      "[1100]\tvalid_0's auc: 0.725143\n",
      "Early stopping, best iteration is:\n",
      "[1020]\tvalid_0's auc: 0.725379\n",
      "[0.7176699661360936, 0.7347776872152024, 0.7253791147896623]\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[100]\tvalid_0's auc: 0.711582\n",
      "[200]\tvalid_0's auc: 0.720378\n",
      "[300]\tvalid_0's auc: 0.723982\n",
      "[400]\tvalid_0's auc: 0.726167\n",
      "[500]\tvalid_0's auc: 0.727811\n",
      "[600]\tvalid_0's auc: 0.728932\n",
      "[700]\tvalid_0's auc: 0.729585\n",
      "[800]\tvalid_0's auc: 0.72982\n",
      "[900]\tvalid_0's auc: 0.729867\n",
      "[1000]\tvalid_0's auc: 0.729843\n",
      "Early stopping, best iteration is:\n",
      "[850]\tvalid_0's auc: 0.729971\n",
      "[0.7176699661360936, 0.7347776872152024, 0.7253791147896623, 0.7299710549896246]\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[100]\tvalid_0's auc: 0.714112\n",
      "[200]\tvalid_0's auc: 0.722273\n",
      "[300]\tvalid_0's auc: 0.725667\n",
      "[400]\tvalid_0's auc: 0.727413\n",
      "[500]\tvalid_0's auc: 0.728927\n",
      "[600]\tvalid_0's auc: 0.729608\n",
      "[700]\tvalid_0's auc: 0.730304\n",
      "[800]\tvalid_0's auc: 0.731009\n",
      "[900]\tvalid_0's auc: 0.73153\n",
      "[1000]\tvalid_0's auc: 0.731801\n",
      "[1100]\tvalid_0's auc: 0.731559\n",
      "Early stopping, best iteration is:\n",
      "[966]\tvalid_0's auc: 0.731835\n",
      "[0.7176699661360936, 0.7347776872152024, 0.7253791147896623, 0.7299710549896246, 0.7318347363704961]\n",
      "OOF-MEAN-AUC:%.6f 0.7279265119002158\n",
      "351\n",
      "Current num of features: 351\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[100]\tvalid_0's auc: 0.700455\n",
      "[200]\tvalid_0's auc: 0.705614\n",
      "[300]\tvalid_0's auc: 0.709241\n",
      "[400]\tvalid_0's auc: 0.711436\n",
      "[500]\tvalid_0's auc: 0.713378\n",
      "[600]\tvalid_0's auc: 0.714858\n",
      "[700]\tvalid_0's auc: 0.715895\n",
      "[800]\tvalid_0's auc: 0.716493\n",
      "[900]\tvalid_0's auc: 0.716892\n",
      "[1000]\tvalid_0's auc: 0.717392\n",
      "[1100]\tvalid_0's auc: 0.717492\n",
      "Early stopping, best iteration is:\n",
      "[1039]\tvalid_0's auc: 0.717566\n",
      "[0.7175662854786486]\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[100]\tvalid_0's auc: 0.71646\n",
      "[200]\tvalid_0's auc: 0.7242\n",
      "[300]\tvalid_0's auc: 0.728427\n",
      "[400]\tvalid_0's auc: 0.730562\n",
      "[500]\tvalid_0's auc: 0.732035\n",
      "[600]\tvalid_0's auc: 0.73349\n",
      "[700]\tvalid_0's auc: 0.734241\n",
      "[800]\tvalid_0's auc: 0.734626\n",
      "[900]\tvalid_0's auc: 0.734984\n",
      "[1000]\tvalid_0's auc: 0.735216\n",
      "[1100]\tvalid_0's auc: 0.735151\n",
      "Early stopping, best iteration is:\n",
      "[989]\tvalid_0's auc: 0.735241\n",
      "[0.7175662854786486, 0.7352413324974436]\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[100]\tvalid_0's auc: 0.711673\n",
      "[200]\tvalid_0's auc: 0.715885\n",
      "[300]\tvalid_0's auc: 0.719382\n",
      "[400]\tvalid_0's auc: 0.721698\n",
      "[500]\tvalid_0's auc: 0.723369\n",
      "[600]\tvalid_0's auc: 0.724786\n",
      "[700]\tvalid_0's auc: 0.725462\n",
      "[800]\tvalid_0's auc: 0.725589\n",
      "[900]\tvalid_0's auc: 0.725837\n",
      "[1000]\tvalid_0's auc: 0.725949\n",
      "[1100]\tvalid_0's auc: 0.725844\n",
      "Early stopping, best iteration is:\n",
      "[977]\tvalid_0's auc: 0.726074\n",
      "[0.7175662854786486, 0.7352413324974436, 0.7260738539898133]\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[100]\tvalid_0's auc: 0.713978\n",
      "[200]\tvalid_0's auc: 0.719842\n",
      "[300]\tvalid_0's auc: 0.723753\n",
      "[400]\tvalid_0's auc: 0.726141\n",
      "[500]\tvalid_0's auc: 0.727\n",
      "[600]\tvalid_0's auc: 0.727948\n",
      "[700]\tvalid_0's auc: 0.728803\n",
      "[800]\tvalid_0's auc: 0.729064\n",
      "[900]\tvalid_0's auc: 0.72916\n",
      "[1000]\tvalid_0's auc: 0.728698\n",
      "Early stopping, best iteration is:\n",
      "[864]\tvalid_0's auc: 0.72931\n",
      "[0.7175662854786486, 0.7352413324974436, 0.7260738539898133, 0.7293103895491416]\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[100]\tvalid_0's auc: 0.71645\n",
      "[200]\tvalid_0's auc: 0.722521\n",
      "[300]\tvalid_0's auc: 0.726774\n",
      "[400]\tvalid_0's auc: 0.729046\n",
      "[500]\tvalid_0's auc: 0.730716\n",
      "[600]\tvalid_0's auc: 0.731798\n",
      "[700]\tvalid_0's auc: 0.732114\n",
      "[800]\tvalid_0's auc: 0.732671\n",
      "[900]\tvalid_0's auc: 0.732572\n",
      "[1000]\tvalid_0's auc: 0.732848\n",
      "[1100]\tvalid_0's auc: 0.732724\n",
      "Early stopping, best iteration is:\n",
      "[1028]\tvalid_0's auc: 0.732912\n",
      "[0.7175662854786486, 0.7352413324974436, 0.7260738539898133, 0.7293103895491416, 0.732912127428787]\n",
      "OOF-MEAN-AUC:%.6f 0.7282207977887668\n"
     ]
    }
   ],
   "source": [
    "def lgb_model(train, target,test, k):\n",
    "    output_preds = 0\n",
    "    feats = [f for f in train.columns if f not in ['user', 'label']]\n",
    "    print('Current num of features:', len(feats))\n",
    "    folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=0)\n",
    "    oof_probs = np.zeros(train.shape[0])\n",
    "    offline_score = []\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    parameters = {\n",
    "        'learning_rate': 0.01,\n",
    "        'reg_lambda':1.9,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'num_leaves': 64,\n",
    "        'max_depth':8,\n",
    "        'feature_fraction': 0.3,\n",
    "        'bagging_fraction': 0.7,\n",
    "        'min_data_in_leaf': 20,\n",
    "        'verbose': -1,\n",
    "        'nthread': 4\n",
    "    }\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(folds.split(train, target)):\n",
    "        train_y, test_y = target[train_index], target[test_index]\n",
    "        train_X, test_X = train[feats].iloc[train_index, :], train[feats].iloc[test_index, :]\n",
    "\n",
    "        dtrain = lgb.Dataset(train_X,\n",
    "                             label=train_y)\n",
    "        dval = lgb.Dataset(test_X,\n",
    "                           label=test_y)\n",
    "        lgb_model = lgb.train(\n",
    "                parameters,\n",
    "                dtrain,\n",
    "                num_boost_round=5000,\n",
    "                valid_sets=[dval],\n",
    "                early_stopping_rounds=150,\n",
    "                verbose_eval=100,\n",
    "        )\n",
    "        \n",
    "        oof_probs[test_index] = lgb_model.predict(test_X[feats], num_iteration=lgb_model.best_iteration)\n",
    "        offline_score.append(lgb_model.best_score['valid_0']['auc'])\n",
    "        print(offline_score)\n",
    "        \n",
    "        output_preds += lgb_model.predict(test[feats], num_iteration=lgb_model.best_iteration)/folds.n_splits\n",
    "\n",
    "        # feature importance\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = lgb_model.feature_importance(importance_type='gain')\n",
    "        fold_importance_df[\"fold\"] = i + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    print('OOF-MEAN-AUC:%.6f',(np.mean(offline_score)))\n",
    "    return np.mean(offline_score),feature_importance_df,output_preds\n",
    "\n",
    "train=data[:47782]\n",
    "test=data.iloc[47782:,:]\n",
    "test=test.reset_index()\n",
    "test=test.drop(columns=['index'])\n",
    "\n",
    "mean_auc,feature_imp,pred=lgb_model(train,train['label'],test,5)\n",
    "\n",
    "table = pd.pivot_table(feature_imp, values=['importance'], index=['feature'], aggfunc={np.mean})\n",
    "table.columns = [' '.join(col).strip() for col in table.columns.values]\n",
    "\n",
    "table=table.sort_values(by=['importance mean'],ascending=False)\n",
    "table=table.reset_index()\n",
    "\n",
    "table.to_csv('feature_imp.csv',index=False)\n",
    "\n",
    "important_feature=np.array(table.loc[:350,'feature']).tolist()\n",
    "train_after_feature_selection=train[important_feature]\n",
    "test_after_feature_selection=test[important_feature]\n",
    "all_after_feature_selection=pd.concat([train_after_feature_selection,test_after_feature_selection],axis=0)\n",
    "print(len(train_after_feature_selection.columns))\n",
    "mean_auc,feature_imp,pred=lgb_model(train_after_feature_selection,train['label'],all_after_feature_selection,5)\n",
    "\n",
    "import datetime\n",
    "result=pd.concat([test['user'],pd.DataFrame(pred[47782:],columns=['prob'])],axis=1)\n",
    "result.to_csv('pred_'+str(len(important_feature))+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4256f736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
